{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7d25b5",
   "metadata": {},
   "source": [
    "**Import the following libraries**\n",
    "\n",
    "- pandas for managing the data.\n",
    "\n",
    "- numpy for mathematical operations.\n",
    "\n",
    "- seaborn for visualizing the data.\n",
    "\n",
    "- matplotlib for visualizing the data.\n",
    "\n",
    "- sklearn for machine learning and machine-learning-pipeline related functions.\n",
    "\n",
    "- scipy for statistical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a707df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e911c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Make                            Model    Price  Year  Kilometer\n",
      "0          Honda              Amaze 1.2 VX i-VTEC   505000  2017      87150\n",
      "1  Maruti Suzuki                  Swift DZire VDI   450000  2014      75000\n",
      "2        Hyundai             i10 Magna 1.2 Kappa2   220000  2011      67000\n",
      "3         Toyota                         Glanza G   799000  2019      37500\n",
      "4         Toyota  Innova 2.4 VX 7 STR [2016-2020]  1950000  2018      69000\n"
     ]
    }
   ],
   "source": [
    "#Reading data from .csv files\n",
    "file_path = 'C:/Users/HP/Desktop/Machine Learning/car_price_data_set.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "#print(data.head())  # Display the first few rows of the dataset\n",
    "print(data.iloc[:5, :5])  # Display the first 5 rows and 5 columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2920136a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 12 fields in line 6, saw 14\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m data = pd.read_csv(file_path, sep=\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# For semicolon-separated values\u001b[39;00m\n\u001b[32m      5\u001b[39m data = pd.read_csv(file_path, sep=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# For tab-separated values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# For space-separated values    \u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#whitespace separated values\u001b[39;00m\n\u001b[32m      9\u001b[39m data = pd.read_csv(file_path, delim_whitespace=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# For whitespace-separated values\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Machine Learning\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Machine Learning\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Machine Learning\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Machine Learning\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 12 fields in line 6, saw 14\n"
     ]
    }
   ],
   "source": [
    "#Useful argumetnts for read_csv\n",
    "# - sep: Specify the delimiter used in the file (default is ',').\n",
    "data = pd.read_csv(file_path, sep=',')  # For comma-separated values\n",
    "data = pd.read_csv(file_path, sep=';')  # For semicolon-separated values\n",
    "data = pd.read_csv(file_path, sep='\\t')  # For tab-separated values\n",
    "data = pd.read_csv(file_path, sep=' ')  # For space-separated values    \n",
    "\n",
    "#whitespace separated values\n",
    "data = pd.read_csv(file_path, delim_whitespace=True)  # For whitespace-separated values\n",
    "\n",
    "#don't use the first row as header\n",
    "data = pd.read_csv(file_path, header=None)  # Treat the first row as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading JSON files\n",
    "data_json = pd.read_json('data/titanic-parquet.json', lines=True)  # Read a JSON file with multiple JSON objects\n",
    "print(data_json.head()) \n",
    "'''for index, row in data_json.iterrows():\n",
    "    print(row.to_dict())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()  # Display information about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f778e6",
   "metadata": {},
   "source": [
    "According to the output above, we have 2059 entries, 0 to 2058, as well as 20 features. The \"Non-Null Count\" column shows the number of non-null entries. If the count is 2059 then there is no missing values for that particular feature. 'Price' is our target or response variable and the rest of the features are our predictor variables.\n",
    "\n",
    "We also have a mix of numerical (3 int64 and 5 float64) and 12 object data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915cb84d",
   "metadata": {},
   "source": [
    "The **describe()** function reveals the statistical information about the **numeric attributes.** To reveal same information about **categorical (object) attributes**, we can use **value_counts()** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Price\"].describe()  # Get summary statistics for the \"Price\" column\n",
    "#data[\"Model\"].value_counts()  # Calculate the mean of the \"Price\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c76ff5",
   "metadata": {},
   "source": [
    "- count: Number of non-null entries (fields/cells in a dataset that contain valid, populated data thus not empty, **NaN, None, or missing**)\n",
    "- mean: average price\n",
    "- std: high standard deviation --> prices vary widely\n",
    "- min: Lowest price value. (Not zero thus no invalid data)\n",
    "- 25%: 25th percentile (25% of prices <= 484k)\n",
    "- 50%: Median\n",
    "- 75%: 75th percentile\n",
    "- max: Highest price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6665054",
   "metadata": {},
   "source": [
    "### **Correlations**\n",
    "\n",
    "Before proceeding with the data cleaning, it is useful to establish a correlation between the target variable (in our case price) and other predictor variables, as some of them might not have any major impact in determining the price of the car and will not be used in the analysis.  \n",
    "\n",
    "There are many ways to discover correlation between the target variable and the rest of the features. \n",
    "\n",
    "Building; \n",
    "- pair plots, \n",
    "- scatter plots, \n",
    "- heat maps, \n",
    "- correlation matrixes are the most common ones. \n",
    "\n",
    "Below, we will use the `corr()` function to list the top features based on the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) (measures how closely two sequences of numbers are correlated). Correlation coefficient can only be calculated on the `numerical attributes (floats and integers)`, therefore, only the numeric attributes will be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ef363",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_in_data = data.select_dtypes(include = ['float64', 'int64'])\n",
    "numerical_in_data_corr = numerical_in_data.corr()['Price']  \n",
    "top_features = numerical_in_data_corr[abs(numerical_in_data_corr) > 0.5].sort_values(ascending=False) #displays pearsons correlation coefficient greater than 0.5\n",
    "print(f\"There is {len(top_features)} strongly correlated values with Price:\\n{top_features}\")\n",
    "#print(numerical_in_data)\n",
    "#print(len(numerical_in_data.columns))\n",
    "#print(numerical_in_data_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecda2c",
   "metadata": {},
   "source": [
    "there are 5 features, with coefficients greater than 0.5, that are strongly correlated with the price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013ffa3",
   "metadata": {},
   "source": [
    "### **Visual inspection** \n",
    "\n",
    "Visually inspecting the correlation between the features and the target with pair plots.\n",
    "\n",
    "use seaborns `_sns.pairplot()_`function\n",
    "\n",
    "Pair plots also help in identifying outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(numerical_in_data.columns), 5):\n",
    "    sns.pairplot(data=numerical_in_data,\n",
    "                x_vars=numerical_in_data.columns[i:i+5],\n",
    "                y_vars=['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01be69",
   "metadata": {},
   "source": [
    "### **Log Transformation**\n",
    "\n",
    "In this section, we are going to inspect whether our 'Price' data is normally distributed. \n",
    "\n",
    "The assumption of the normal distribution must be met in order to perform any type of regression analysis. \n",
    "\n",
    "There are several ways to check for this assumption, however here, we will use the visual method, by plotting the 'Price' distribution using the `histtplot()` function from the seaborn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e100f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_price_plot = sns.histplot(data['Price'], kde=True)\n",
    "initial_price_plot = sns.histplot(data['Price'], kde=True, bins=50) #kde=True adds a kernel density estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559ca3a",
   "metadata": {},
   "source": [
    "\n",
    "Kernel Density Estimate (KDE) is a non-parametric way to estimate the probability density function of a random variable. It is used to visualize the distribution of data and identify patterns, such as peaks or gaps, in the data. KDE smooths the data points using a kernel function (commonly Gaussian) to create a continuous curve that represents the density of the data.\n",
    "\n",
    "As the plot shows, our 'Price' deviates from the normal distribution. It has a longer tail to the right, so we call it a positive skew.\n",
    "\n",
    "In statistics **skewness** is a measure of asymmetry of the distribution. \n",
    "\n",
    "In addition to skewness, there is also a **kurtosis**, parameter which refers to the pointedness of a peak in the distribution curve. \n",
    "\n",
    "Both `skewness and kurtosis` are frequently used together to characterize the distribution of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use skew() to check for skewness level of Price\n",
    "print(f\"Skewness of Price: {data['Price'].skew()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56276be5",
   "metadata": {},
   "source": [
    "\n",
    "The range of skewness for a;\n",
    "\n",
    "- fairly symmetrical bell curve distribution is between -0.5 and 0.5\n",
    "\n",
    "- moderate skewness is -0.5 to -1.0 and 0.5 to 1.0\n",
    "\n",
    "- highly skewed distribution is < -1.0 and > 1.0. \n",
    "\n",
    "In our case, we have **~4.9**, so it is considered highly skewed data.\n",
    "\n",
    "Now, we can try to transform our data, so it looks more normally distributed. We can use the np.log() function from the numpy library to perform log transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformed_price = np.log(data['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_price_plot = sns.histplot(log_transformed_price, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % (log_transformed_price).skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79cd635",
   "metadata": {},
   "source": [
    "As we can see, the log method transformed the 'Price' distribution into a more symmetrical bell curve and the skewness level now is 0.485859, well within the range.\n",
    "\n",
    "There are other ways to correct for skewness of the data. For example, `Square Root Transform (np.sqrt)` and the `Box-Cox Transform (stats.boxcox)` from the scipy stats library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20af17",
   "metadata": {},
   "source": [
    "**Handling Duplicates**\n",
    "\n",
    "We will use pandas duplicated() function and search by the 'Model' column, which contains a unique index number for each entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any duplicate indexes in the dataset\n",
    "data.index.is_unique  # Returns True if all index values are unique, False otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = data[data.duplicated(['Model'])]\n",
    "duplicate\n",
    "#duplicates = data[data.duplicated(subset=['Make', 'Model', 'Year', 'Kilometer'], keep=False)]\n",
    "#duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ae392f",
   "metadata": {},
   "source": [
    "As we can see, there are 1009 duplicate rows in this dataset. To remove them, we can use pandas `drop_duplicates()` function. By default, it removes all duplicate rows based on all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_removed = data.drop_duplicates()\n",
    "dup_removed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dad4b2",
   "metadata": {},
   "source": [
    "**Handling the missing values**\n",
    "\n",
    "For easier detection of missing values, pandas provides the `isna()`, `isnull()`, and `notna()` 'functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491fa2fe",
   "metadata": {},
   "source": [
    "To summarize all the missing values in our dataset, we will use `isnull()` function. Then, we will add them all up, by using `sum()` function, sort them with `sort_values()` function, and plot them, using the `bar plot` function from the `matplotlib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "total_select = total.head(20)\n",
    "total_select.plot(kind=\"bar\", figsize = (8,6), fontsize = 10)\n",
    "\n",
    "plt.xlabel(\"Columns\", fontsize = 20)\n",
    "plt.ylabel(\"Count\", fontsize = 20)\n",
    "plt.title(\"Total Missing Values\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928893e7",
   "metadata": {},
   "source": [
    "**Dropping missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6743f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with missing values\n",
    "data_dropped = data.dropna()\n",
    "print(f\"Number of rows after dropping missing values: {len(data_dropped)}\")\n",
    "\n",
    "# #drop all rows with missing values in the 'Price' column\n",
    "# data_dropped_price = data.dropna(subset=['Price'])\n",
    "# print(f\"Number of rows after dropping missing values in 'Price': {len(data_dropped_price)}\")\n",
    "\n",
    "# #drop a whole column with missing values\n",
    "# data_dropped_column = data.drop(columns=['Make'])\n",
    "# print(f\"Number of columns after dropping 'Make': {len(data_dropped_column.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633aed6",
   "metadata": {},
   "source": [
    "**Filling missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing values with the mean of the column\n",
    "# data_filled_mean = data.fillna(data.mean())\n",
    "# print(f\"Number of rows after filling missing values with mean: {len(data_filled_mean)}\")        \n",
    "\n",
    "# fill missing values with the median of the column\n",
    "# data_filled_median = data.fillna(data.median())\n",
    "# print(f\"Number of rows after filling missing values with median: {len(data_filled_median)}\")    \n",
    "\n",
    "# #fill missing values with the mode of the column\n",
    "# data_filled_mode = data.fillna(data.mode().iloc[0])\n",
    "# print(f\"Number of rows after filling missing values with mode: {len(data_filled_mode)}\")    \n",
    "\n",
    "#fill missing values with a constant value\n",
    "data_filled_constant = data.fillna(0)\n",
    "print(f\"Number of rows after filling missing values with constant: {len(data_filled_constant)}\")\n",
    "\n",
    "# #fill missing values with the previous value in the column\n",
    "# data_filled_ffill = data.fillna(method='ffill')\n",
    "# print(f\"Number of rows after filling missing values with forward fill: {len(data_filled_ffill)}\")   \n",
    "\n",
    "# #fill missing values in a specific column with the mean of that column\n",
    "# data['Price'] = data['Price'].fillna(data['Price'].mean())\n",
    "# print(f\"Number of rows after filling missing values in 'Price' with mean: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a0bf3",
   "metadata": {},
   "source": [
    "### **Feature scaling**\n",
    "\n",
    "One of the most important transformations we need to apply to our data is feature scaling.  There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\n",
    "\n",
    "Min-max scaling (or normalization) is the simplest: values are shifted and rescaled so they end up ranging from 0 to 1. This is done by subtracting the min value and dividing by the max minus min.\n",
    "\n",
    "Standardization is different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation, so that the resulting distribution has unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f409d6",
   "metadata": {},
   "source": [
    "Scikit-learn library provides `MinMaxScaler` for normalization and `StandardScaler` for standardization needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "\n",
    "normalized_data = MinMaxScaler().fit_transform(numerical_in_data)\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the data\n",
    "standardized_data = StandardScaler().fit_transform(numerical_in_data)\n",
    "standardized_data\n",
    "\n",
    "# #standidze a particular column\n",
    "# standardized_price = StandardScaler().fit_transform(data[['Price']])\n",
    "# standardized_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75667383",
   "metadata": {},
   "source": [
    "### **Handling outliers**\n",
    "\n",
    "In statistics, an outlier is an observation point that is distant from other observations. An outlier can be due to some mistakes in data collection or recording, or due to natural high variability of data points. How to treat an outlier highly depends on our data or the type of analysis to be performed. Outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n",
    "\n",
    "There are many ways to discover outliers in our data. We can do Uni-variate analysis (using one variable analysis) or Multi-variate analysis (using two or more variables). One of the simplest ways to detect an outlier is to inspect the data visually, by making box plots or scatter plots. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d36f16",
   "metadata": {},
   "source": [
    "### **Univariate Analysis**\n",
    "\n",
    "A box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles. Outliers may be plotted as individual points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa39bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use box plot for univariate analysis of price\n",
    "sns.boxplot(x=data['Price'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot box plot for all numerical columns\n",
    "for col in numerical_in_data.columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(x=data[col])\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot box plot for features above 0.5 correlation with Price\n",
    "for col in top_features.index:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(x=data[col])\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21bd7e7",
   "metadata": {},
   "source": [
    "\n",
    "As we can see from these plots, we have some points that are plotted outside the box plot area that greatly deviate from the rest of the population. Whether to remove or keep them will greatly depend on the understanding of our data and the type of analysis to be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cff075",
   "metadata": {},
   "source": [
    "### **Bi-variate analysis**\n",
    "\n",
    "Next, we will look at the bi-variate analysis of the two features, the car price, 'Price', and the engine capacity, 'Engine (cc)', and plot the scatter plot of the relationship between these two parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cb51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve columns with Pearson correlation greater than 0.5 and print them\n",
    "correlated_columns = top_features.index.tolist()\n",
    "print(\"Columns with Pearson correlation greater than 0.5:\", correlated_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1946b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_area = data.plot.scatter(x='Engine (cc)',\n",
    "                      y='Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581eddbb",
   "metadata": {},
   "source": [
    "From the above graph, we can see values that deviate from the general trend both on the x and y axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59710f",
   "metadata": {},
   "source": [
    "### **Deleting outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd20495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first sort the data by 'Engine (cc)' and select the last 3\n",
    "data.sort_values(by = 'Engine (cc)', ascending = False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d056413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the pandas drop() function to delete the outliers\n",
    "outliers_droped = data.drop(data.index[[977,1246,1369]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c905263",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_price_area = outliers_droped.plot.scatter(x='Engine (cc)',\n",
    "                      y='Price')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
